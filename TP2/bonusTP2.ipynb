{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "bonusTP2.ipynb",
   "provenance": [],
   "collapsed_sections": [
    "HIqZkC3iKtjz",
    "qfm4sXZHLAdi",
    "ORxU4c7KKepG"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HIqZkC3iKtjz"
   },
   "source": [
    "# **TP2 (BONUS part) : construction, apprentissage et évaluation de premiers classifieurs supervisés**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYQWq85MNl1Z"
   },
   "source": [
    "# 3.Gaussian Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V62EY7d3TiSn"
   },
   "source": [
    "## Model overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVTyrDHOTKv4"
   },
   "source": [
    "Below, we will develop the Gaussian Naive Bayes classifier in a multi-class classification task. To understand its principle, let's reverse the classical machine learning workflow and look at how this classifier will predict the class $C_i$ of a new data sample $(x_1,...,x_n)$ representing some $n$ features (independent variables). Based on the [Baye's theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem), this prediction results from the following conditional probability \n",
    "\n",
    "\\begin{equation} \n",
    "P(C_i | x_1,...,x_n ) = P(x_1,...,x_n | C_i ) * P( C_i ) \\tag{eq. 4.1}\n",
    "\\end{equation}\n",
    "\n",
    "It is accompanied with a set of strong assumptions on these probability distributions, summed up below:\n",
    "\n",
    "-  **Data columns are conditionally independent of each other**, i.e. the input variables are treated separately, that is\n",
    "\n",
    "\\begin{equation}\n",
    "P(x_1,...,x_n | C_i) = P(x_1| C_i) *...* P(x_n| C_i) \\tag{eq. 4.2}\n",
    "\\end{equation}\n",
    "\n",
    "-  **Data are normally distributed**, i.e. the distribution of each input attribute (i.e. each column of our data) $P(x_k | C_i)$ will be modeled as a gaussian distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfY2cu5RV2hE"
   },
   "source": [
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4-F4_PpXGT3"
   },
   "source": [
    "To illustrate this classifier on a multi-class single-label classification task, we will be using the famous [Iris](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html) dataset (see its documentation to have some details on it). The code cell below downloads it into a pandas DataFrame and compute the training and test variables."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PYvnVTEwGmuJ",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "outputId": "e72826a2-6a4a-43de-8dd7-3a2edff5ef95"
   },
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "dataset = load_iris()\n",
    "\n",
    "y = dataset['target']\n",
    "X = pd.DataFrame(dataset.data , columns = dataset['feature_names'])\n",
    "y = pd.Series(y)\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X , y,test_size=0.2 , random_state = 7)\n",
    "\n",
    "X.head()"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n0                5.1               3.5                1.4               0.2\n1                4.9               3.0                1.4               0.2\n2                4.7               3.2                1.3               0.2\n3                4.6               3.1                1.5               0.2\n4                5.0               3.6                1.4               0.2",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal length (cm)</th>\n      <th>sepal width (cm)</th>\n      <th>petal length (cm)</th>\n      <th>petal width (cm)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rGA4U39V9v5A",
    "outputId": "963fb073-fa09-4567-c6bb-6a35219ae355"
   },
   "source": [
    "np.unique(dataset['target'])"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 1, 2])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "070MBsa4H4dI"
   },
   "source": [
    "## Model training: calculate P(X1,...,Xn |class) and P(class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PCPbWU-Iksw"
   },
   "source": [
    "To calculate the probability of data by the class they belong to, i.e. P(data |class), we need to 1) separate our training data by class and 2) calculate  the mean and standard deviation statistics, $\\mathbf{\\mu}$ and $\\Sigma$, for each column grouped by class. The latter is needed to train the normal distribution of each attribute value given a class.\n",
    "\n",
    "**Question 3.1** Using the [groupby](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html) function of pandas dataframes, implement these two operations into a single line of code to compute $\\mathbf{\\mu}$ and then $\\Sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def calc_stat(X_train, y_train):\n",
    "    class_groups = X_train.groupby(y_train)\n",
    "    mu = class_groups.apply(lambda x: x.mean())\n",
    "    sigma = class_groups.apply(lambda x: x.std())\n",
    "    return mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0GAeo-zYms0"
   },
   "source": [
    "**Question 3.2** Also we require to compute the class prior probability, which is simply the number of class elements divided by the total number of elements in the train set. Still using the `groupby()` method, as well as the `lambda` operator within a `apply()` method, compute this probability into a single line of code. \n",
    "\n",
    "*tips*: to help you with the `apply()` and `lambda()` functions, first try to understand this toy [example](https://pandas.pydata.org/pandas-docs/version/0.22.0/generated/pandas.core.groupby.GroupBy.apply.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.358333\n",
      "1    0.316667\n",
      "2    0.325000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def calc_prior(y_train):\n",
    "    class_groups = y_train.groupby(y_train)\n",
    "    prior = class_groups.apply(lambda x: x.count()/len(y_train))\n",
    "    return prior\n",
    "\n",
    "print(calc_prior(y_train))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74A4YGNmI2mG"
   },
   "source": [
    "## Model test: calculate P( class | X1 , ... , Xn) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFpuN8mHSCwF"
   },
   "source": [
    "**Question 3.3** Before going any further, let's first be sure you well understand the meaning of the term $P(x_k | C_l)$ in eq. 4.2.\n",
    "\n",
    "With one sentence, can you explain what this term will compute during our test phase ? Using the function `univariate_normal`, calculate the values of this term for $x_k = [1,2,0]$, $\\mu = [1,1,1]$ and $\\Sigma = [1,1,1]$. Comment the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "0    0.398942\n1    0.241971\n2    0.241971\ndtype: float64"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1,2,0]\n",
    "Cl = pd.DataFrame({'mean': [1,1,1], 'std': [1,1,1]})\n",
    "\n",
    "def univariate_normal(x, mean, std):\n",
    "    return (1 / (np.sqrt(2 * np.pi) * std)) * np.exp(-0.5 * ((x - mean) / std)**2)\n",
    "\n",
    "univariate_normal(x, Cl['mean'], Cl['std'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtJ6EXP_fjz4"
   },
   "source": [
    "**Question 3.4** Implement the calculation of P( class | X1 , ... , Xn) based on equations 4.1 and 4.2.\n",
    "\n",
    "*Tips* : we recommend the use of three `for` loops on 1) test samples, 2) possible class and 3) each sample column. For the loop 3), you can use the `enumerate` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 0, 1, 1, 0, 2, 1, 0, 1, 2, 1, 0, 2, 0, 2, 2, 2, 0, 0, 1, 2, 1, 1, 2, 2, 1, 1, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "def predict(X_test, mu, sigma, prior):\n",
    "    y_pred = []\n",
    "    for i in range(len(X_test)):\n",
    "        proba = []\n",
    "        for j in range(len(mu)):\n",
    "            proba.append(prior[j])\n",
    "            for k in range(len(X_test.columns)):\n",
    "                proba[j] *= univariate_normal(X_test.iloc[i,k], mu.iloc[j,k], sigma.iloc[j,k])\n",
    "        y_pred.append(np.argmax(proba))\n",
    "    return y_pred\n",
    "\n",
    "mu, sigma = calc_stat(X_train, y_train)\n",
    "prior = calc_prior(y_train)\n",
    "y_pred = predict(X_test, mu, sigma, prior)\n",
    "\n",
    "print(y_pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Wr9sa29LHM4"
   },
   "source": [
    "**Question 3.5** Verify that your model performs similarly as the sklearn `GaussianNB` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# verify that the two models give the same results\n",
    "print(np.array_equal(y_pred, y_pred))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORxU4c7KKepG"
   },
   "source": [
    "# SOURCES\n",
    "\n",
    "## Naive bayes\n",
    "\n",
    "- https://machinelearningmastery.com/naive-bayes-classifier-scratch-python/\n",
    "\n",
    "- https://towardsdatascience.com/implementing-naive-bayes-in-2-minutes-with-python-3ecd788803fe\n"
   ]
  }
 ]
}
